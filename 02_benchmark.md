

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

------------------------------------------------------------------------

<a
href="https://github.com/numb3r33/rlm/blob/main/rlm/benchmarks.py#L14"
target="_blank" style="float:right; font-size:smaller">source</a>

### benchmark_vanilla

>  benchmark_vanilla (context, query, model='gpt-4', base_url=None)

\*Benchmark vanilla LLM approach (direct call with full context).

Returns dict with: method, time, answer, tokens (if available)\*

------------------------------------------------------------------------

<a
href="https://github.com/numb3r33/rlm/blob/main/rlm/benchmarks.py#L39"
target="_blank" style="float:right; font-size:smaller">source</a>

### benchmark_rlm

>  benchmark_rlm (context, query, model='gpt-4', base_url=None,
>                     max_steps=100)

\*Benchmark RLM approach with REPL environment and recursive calls.

Returns dict with: method, time, answer, tokens (if available)\*

``` python
with open('../context/illiad.txt', 'r') as f:
    context = f.read()
```

``` python
q = "What gifts does Agamemnon offer Achilles?"
```

``` python
benchmark_vanilla(context, 
                  query=q, 
                  model="openai/openai/gpt-oss-120b", 
                  base_url="https://<your_gateway>.com"
                 )
```


    Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
    LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.

    BadRequestError: litellm.BadRequestError: OpenAIException - litellm.BadRequestError: OpenAIException - max_tokens must be at least 1, got -144216.. Received Model Group=openai/gpt-oss-120b
    Available Model Group Fallbacks=None
    [0;31m---------------------------------------------------------------------------[0m
    [0;31mBadRequestError[0m                           Traceback (most recent call last)
    File [0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/llms/openai/openai.py:745[0m, in [0;36mOpenAIChatCompletion.completion[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params, shared_session)[0m
    [1;32m    744[0m             [38;5;28;01melse[39;00m:
    [0;32m--> 745[0m                 [38;5;28;01mraise[39;00m e
    [1;32m    746[0m [38;5;28;01mexcept[39;00m OpenAIError [38;5;28;01mas[39;00m e:

    File [0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/llms/openai/openai.py:673[0m, in [0;36mOpenAIChatCompletion.completion[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params, shared_session)[0m
    [1;32m    659[0m logging_obj[38;5;241m.[39mpre_call(
    [1;32m    660[0m     [38;5;28minput[39m[38;5;241m=[39mmessages,
    [1;32m    661[0m     api_key[38;5;241m=[39mopenai_client[38;5;241m.[39mapi_key,
    [0;32m   (...)[0m
    [1;32m    667[0m     },
    [1;32m    668[0m )
    [1;32m    670[0m (
    [1;32m    671[0m     headers,
    [1;32m    672[0m     response,
    [0;32m--> 673[0m ) [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mmake_sync_openai_chat_completion_request[49m[43m([49m
    [1;32m    674[0m [43m    [49m[43mopenai_client[49m[38;5;241;43m=[39;49m[43mopenai_client[49m[43m,[49m
    [1;32m    675[0m [43m    [49m[43mdata[49m[38;5;241;43m=[39;49m[43mdata[49m[43m,[49m
    [1;32m    676[0m [43m    [49m[43mtimeout[49m[38;5;241;43m=[39;49m[43mtimeout[49m[43m,[49m
    [1;32m    677[0m [43m    [49m[43mlogging_obj[49m[38;5;241;43m=[39;49m[43mlogging_obj[49m[43m,[49m
    [1;32m    678[0m [43m[49m[43m)[49m
    [1;32m    680[0m logging_obj[38;5;241m.[39mmodel_call_details[[38;5;124m"[39m[38;5;124mresponse_headers[39m[38;5;124m"[39m] [38;5;241m=[39m headers

    File [0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:237[0m, in [0;36mtrack_llm_api_timing.<locals>.decorator.<locals>.sync_wrapper[0;34m(*args, **kwargs)[0m
    [1;32m    236[0m [38;5;28;01mtry[39;00m:
    [0;32m--> 237[0m     result [38;5;241m=[39m [43mfunc[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
    [1;32m    238[0m     [38;5;28;01mreturn[39;00m result

    File [0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/llms/openai/openai.py:489[0m, in [0;36mOpenAIChatCompletion.make_sync_openai_chat_completion_request[0;34m(self, openai_client, data, timeout, logging_obj)[0m
    [1;32m    488[0m [38;5;28;01melse[39;00m:
    [0;32m--> 489[0m     [38;5;28;01mraise[39;00m e

    File [0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/llms/openai/openai.py:471[0m, in [0;36mOpenAIChatCompletion.make_sync_openai_chat_completion_request[0;34m(self, openai_client, data, timeout, logging_obj)[0m
    [1;32m    470[0m [38;5;28;01mtry[39;00m:
    [0;32m--> 471[0m     raw_response [38;5;241m=[39m [43mopenai_client[49m[38;5;241;43m.[39;49m[43mchat[49m[38;5;241;43m.[39;49m[43mcompletions[49m[38;5;241;43m.[39;49m[43mwith_raw_response[49m[38;5;241;43m.[39;49m[43mcreate[49m[43m([49m
    [1;32m    472[0m [43m        [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mdata[49m[43m,[49m[43m [49m[43mtimeout[49m[38;5;241;43m=[39;49m[43mtimeout[49m
    [1;32m    473[0m [43m    [49m[43m)[49m
    [1;32m    475[0m     [38;5;28;01mif[39;00m [38;5;28mhasattr[39m(raw_response, [38;5;124m"[39m[38;5;124mheaders[39m[38;5;124m"[39m):

    File [0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/openai/_legacy_response.py:364[0m, in [0;36mto_raw_response_wrapper.<locals>.wrapped[0;34m(*args, **kwargs)[0m
    [1;32m    362[0m kwargs[[38;5;124m"[39m[38;5;124mextra_headers[39m[38;5;124m"[39m] [38;5;241m=[39m extra_headers
    [0;32m--> 364[0m [38;5;28;01mreturn[39;00m cast(LegacyAPIResponse[R], [43mfunc[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m)

    File [0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/openai/_utils/_utils.py:286[0m, in [0;36mrequired_args.<locals>.inner.<locals>.wrapper[0;34m(*args, **kwargs)[0m
    [1;32m    285[0m     [38;5;28;01mraise[39;00m [38;5;167;01mTypeError[39;00m(msg)
    [0;32m--> 286[0m [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

    File [0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1189[0m, in [0;36mCompletions.create[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)[0m
    [1;32m   1188[0m validate_response_format(response_format)
    [0;32m-> 1189[0m [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_post[49m[43m([49m
    [1;32m   1190[0m [43m    [49m[38;5;124;43m"[39;49m[38;5;124;43m/chat/completions[39;49m[38;5;124;43m"[39;49m[43m,[49m
    [1;32m   1191[0m [43m    [49m[43mbody[49m[38;5;241;43m=[39;49m[43mmaybe_transform[49m[43m([49m
    [1;32m   1192[0m [43m        [49m[43m{[49m
    [1;32m   1193[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mmessages[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mmessages[49m[43m,[49m
    [1;32m   1194[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mmodel[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mmodel[49m[43m,[49m
    [1;32m   1195[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43maudio[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43maudio[49m[43m,[49m
    [1;32m   1196[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mfrequency_penalty[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mfrequency_penalty[49m[43m,[49m
    [1;32m   1197[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mfunction_call[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mfunction_call[49m[43m,[49m
    [1;32m   1198[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mfunctions[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mfunctions[49m[43m,[49m
    [1;32m   1199[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mlogit_bias[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mlogit_bias[49m[43m,[49m
    [1;32m   1200[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mlogprobs[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mlogprobs[49m[43m,[49m
    [1;32m   1201[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mmax_completion_tokens[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mmax_completion_tokens[49m[43m,[49m
    [1;32m   1202[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mmax_tokens[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mmax_tokens[49m[43m,[49m
    [1;32m   1203[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mmetadata[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mmetadata[49m[43m,[49m
    [1;32m   1204[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mmodalities[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mmodalities[49m[43m,[49m
    [1;32m   1205[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mn[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mn[49m[43m,[49m
    [1;32m   1206[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mparallel_tool_calls[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mparallel_tool_calls[49m[43m,[49m
    [1;32m   1207[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mprediction[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mprediction[49m[43m,[49m
    [1;32m   1208[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mpresence_penalty[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mpresence_penalty[49m[43m,[49m
    [1;32m   1209[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mprompt_cache_key[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mprompt_cache_key[49m[43m,[49m
    [1;32m   1210[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mprompt_cache_retention[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mprompt_cache_retention[49m[43m,[49m
    [1;32m   1211[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mreasoning_effort[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mreasoning_effort[49m[43m,[49m
    [1;32m   1212[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mresponse_format[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mresponse_format[49m[43m,[49m
    [1;32m   1213[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43msafety_identifier[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43msafety_identifier[49m[43m,[49m
    [1;32m   1214[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mseed[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mseed[49m[43m,[49m
    [1;32m   1215[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mservice_tier[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mservice_tier[49m[43m,[49m
    [1;32m   1216[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mstop[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mstop[49m[43m,[49m
    [1;32m   1217[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mstore[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mstore[49m[43m,[49m
    [1;32m   1218[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mstream[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mstream[49m[43m,[49m
    [1;32m   1219[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mstream_options[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mstream_options[49m[43m,[49m
    [1;32m   1220[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mtemperature[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mtemperature[49m[43m,[49m
    [1;32m   1221[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mtool_choice[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mtool_choice[49m[43m,[49m
    [1;32m   1222[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mtools[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mtools[49m[43m,[49m
    [1;32m   1223[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mtop_logprobs[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mtop_logprobs[49m[43m,[49m
    [1;32m   1224[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mtop_p[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mtop_p[49m[43m,[49m
    [1;32m   1225[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43muser[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43muser[49m[43m,[49m
    [1;32m   1226[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mverbosity[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mverbosity[49m[43m,[49m
    [1;32m   1227[0m [43m            [49m[38;5;124;43m"[39;49m[38;5;124;43mweb_search_options[39;49m[38;5;124;43m"[39;49m[43m:[49m[43m [49m[43mweb_search_options[49m[43m,[49m
    [1;32m   1228[0m [43m        [49m[43m}[49m[43m,[49m
    [1;32m   1229[0m [43m        [49m[43mcompletion_create_params[49m[38;5;241;43m.[39;49m[43mCompletionCreateParamsStreaming[49m
    [1;32m   1230[0m [43m        [49m[38;5;28;43;01mif[39;49;00m[43m [49m[43mstream[49m
    [1;32m   1231[0m [43m        [49m[38;5;28;43;01melse[39;49;00m[43m [49m[43mcompletion_create_params[49m[38;5;241;43m.[39;49m[43mCompletionCreateParamsNonStreaming[49m[43m,[49m
    [1;32m   1232[0m [43m    [49m[43m)[49m[43m,[49m
    [1;32m   1233[0m [43m    [49m[43moptions[49m[38;5;241;43m=[39;49m[43mmake_request_options[49m[43m([49m
    [1;32m   1234[0m [43m        [49m[43mextra_headers[49m[38;5;241;43m=[39;49m[43mextra_headers[49m[43m,[49m[43m [49m[43mextra_query[49m[38;5;241;43m=[39;49m[43mextra_query[49m[43m,[49m[43m [49m[43mextra_body[49m[38;5;241;43m=[39;49m[43mextra_body[49m[43m,[49m[43m [49m[43mtimeout[49m[38;5;241;43m=[39;49m[43mtimeout[49m
    [1;32m   1235[0m [43m    [49m[43m)[49m[43m,[49m
    [1;32m   1236[0m [43m    [49m[43mcast_to[49m[38;5;241;43m=[39;49m[43mChatCompletion[49m[43m,[49m
    [1;32m   1237[0m [43m    [49m[43mstream[49m[38;5;241;43m=[39;49m[43mstream[49m[43m [49m[38;5;129;43;01mor[39;49;00m[43m [49m[38;5;28;43;01mFalse[39;49;00m[43m,[49m
    [1;32m   1238[0m [43m    [49m[43mstream_cls[49m[38;5;241;43m=[39;49m[43mStream[49m[43m[[49m[43mChatCompletionChunk[49m[43m][49m[43m,[49m
    [1;32m   1239[0m [43m[49m[43m)[49m

    File [0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/openai/_base_client.py:1259[0m, in [0;36mSyncAPIClient.post[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)[0m
    [1;32m   1256[0m opts [38;5;241m=[39m FinalRequestOptions[38;5;241m.[39mconstruct(
    [1;32m   1257[0m     method[38;5;241m=[39m[38;5;124m"[39m[38;5;124mpost[39m[38;5;124m"[39m, url[38;5;241m=[39mpath, json_data[38;5;241m=[39mbody, files[38;5;241m=[39mto_httpx_files(files), [38;5;241m*[39m[38;5;241m*[39moptions
    [1;32m   1258[0m )
    [0;32m-> 1259[0m [38;5;28;01mreturn[39;00m cast(ResponseT, [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mrequest[49m[43m([49m[43mcast_to[49m[43m,[49m[43m [49m[43mopts[49m[43m,[49m[43m [49m[43mstream[49m[38;5;241;43m=[39;49m[43mstream[49m[43m,[49m[43m [49m[43mstream_cls[49m[38;5;241;43m=[39;49m[43mstream_cls[49m[43m)[49m)

    File [0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/openai/_base_client.py:1047[0m, in [0;36mSyncAPIClient.request[0;34m(self, cast_to, options, stream, stream_cls)[0m
    [1;32m   1046[0m     log[38;5;241m.[39mdebug([38;5;124m"[39m[38;5;124mRe-raising status error[39m[38;5;124m"[39m)
    [0;32m-> 1047[0m     [38;5;28;01mraise[39;00m [38;5;28mself[39m[38;5;241m.[39m_make_status_error_from_response(err[38;5;241m.[39mresponse) [38;5;28;01mfrom[39;00m [38;5;28;01mNone[39;00m
    [1;32m   1049[0m [38;5;28;01mbreak[39;00m

    [0;31mBadRequestError[0m: Error code: 400 - {'error': {'message': 'litellm.BadRequestError: OpenAIException - max_tokens must be at least 1, got -144216.. Received Model Group=openai/gpt-oss-120b\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '400'}}

    During handling of the above exception, another exception occurred:

    [0;31mOpenAIError[0m                               Traceback (most recent call last)
    File [0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/main.py:2158[0m, in [0;36mcompletion[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)[0m
    [1;32m   2152[0m     logging[38;5;241m.[39mpost_call(
    [1;32m   2153[0m         [38;5;28minput[39m[38;5;241m=[39mmessages,
    [1;32m   2154[0m         api_key[38;5;241m=[39mapi_key,
    [1;32m   2155[0m         original_response[38;5;241m=[39m[38;5;28mstr[39m(e),
    [1;32m   2156[0m         additional_args[38;5;241m=[39m{[38;5;124m"[39m[38;5;124mheaders[39m[38;5;124m"[39m: headers},
    [1;32m   2157[0m     )
    [0;32m-> 2158[0m     [38;5;28;01mraise[39;00m e
    [1;32m   2160[0m [38;5;28;01mif[39;00m optional_params[38;5;241m.[39mget([38;5;124m"[39m[38;5;124mstream[39m[38;5;124m"[39m, [38;5;28;01mFalse[39;00m):
    [1;32m   2161[0m     [38;5;66;03m## LOGGING[39;00m

    File [0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/main.py:2130[0m, in [0;36mcompletion[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)[0m
    [1;32m   2129[0m     [38;5;28;01melse[39;00m:
    [0;32m-> 2130[0m         response [38;5;241m=[39m [43mopenai_chat_completions[49m[38;5;241;43m.[39;49m[43mcompletion[49m[43m([49m
    [1;32m   2131[0m [43m            [49m[43mmodel[49m[38;5;241;43m=[39;49m[43mmodel[49m[43m,[49m
    [1;32m   2132[0m [43m            [49m[43mmessages[49m[38;5;241;43m=[39;49m[43mmessages[49m[43m,[49m
    [1;32m   2133[0m [43m            [49m[43mheaders[49m[38;5;241;43m=[39;49m[43mheaders[49m[43m,[49m
    [1;32m   2134[0m [43m            [49m[43mmodel_response[49m[38;5;241;43m=[39;49m[43mmodel_response[49m[43m,[49m
    [1;32m   2135[0m [43m            [49m[43mprint_verbose[49m[38;5;241;43m=[39;49m[43mprint_verbose[49m[43m,[49m
    [1;32m   2136[0m [43m            [49m[43mapi_key[49m[38;5;241;43m=[39;49m[43mapi_key[49m[43m,[49m
    [1;32m   2137[0m [43m            [49m[43mapi_base[49m[38;5;241;43m=[39;49m[43mapi_base[49m[43m,[49m
    [1;32m   2138[0m [43m            [49m[43macompletion[49m[38;5;241;43m=[39;49m[43macompletion[49m[43m,[49m
    [1;32m   2139[0m [43m            [49m[43mlogging_obj[49m[38;5;241;43m=[39;49m[43mlogging[49m[43m,[49m
    [1;32m   2140[0m [43m            [49m[43moptional_params[49m[38;5;241;43m=[39;49m[43moptional_params[49m[43m,[49m
    [1;32m   2141[0m [43m            [49m[43mlitellm_params[49m[38;5;241;43m=[39;49m[43mlitellm_params[49m[43m,[49m
    [1;32m   2142[0m [43m            [49m[43mlogger_fn[49m[38;5;241;43m=[39;49m[43mlogger_fn[49m[43m,[49m
    [1;32m   2143[0m [43m            [49m[43mtimeout[49m[38;5;241;43m=[39;49m[43mtimeout[49m[43m,[49m[43m  [49m[38;5;66;43;03m# type: ignore[39;49;00m
    [1;32m   2144[0m [43m            [49m[43mcustom_prompt_dict[49m[38;5;241;43m=[39;49m[43mcustom_prompt_dict[49m[43m,[49m
    [1;32m   2145[0m [43m            [49m[43mclient[49m[38;5;241;43m=[39;49m[43mclient[49m[43m,[49m[43m  [49m[38;5;66;43;03m# pass AsyncOpenAI, OpenAI client[39;49;00m
    [1;32m   2146[0m [43m            [49m[43morganization[49m[38;5;241;43m=[39;49m[43morganization[49m[43m,[49m
    [1;32m   2147[0m [43m            [49m[43mcustom_llm_provider[49m[38;5;241;43m=[39;49m[43mcustom_llm_provider[49m[43m,[49m
    [1;32m   2148[0m [43m            [49m[43mshared_session[49m[38;5;241;43m=[39;49m[43mshared_session[49m[43m,[49m
    [1;32m   2149[0m [43m        [49m[43m)[49m
    [1;32m   2150[0m [38;5;28;01mexcept[39;00m [38;5;167;01mException[39;00m [38;5;28;01mas[39;00m e:
    [1;32m   2151[0m     [38;5;66;03m## LOGGING - log the original exception returned[39;00m

    File [0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/llms/openai/openai.py:756[0m, in [0;36mOpenAIChatCompletion.completion[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params, shared_session)[0m
    [1;32m    755[0m     error_headers [38;5;241m=[39m [38;5;28mgetattr[39m(error_response, [38;5;124m"[39m[38;5;124mheaders[39m[38;5;124m"[39m, [38;5;28;01mNone[39;00m)
    [0;32m--> 756[0m [38;5;28;01mraise[39;00m OpenAIError(
    [1;32m    757[0m     status_code[38;5;241m=[39mstatus_code,
    [1;32m    758[0m     message[38;5;241m=[39merror_text,
    [1;32m    759[0m     headers[38;5;241m=[39merror_headers,
    [1;32m    760[0m     body[38;5;241m=[39merror_body,
    [1;32m    761[0m )

    [0;31mOpenAIError[0m: Error code: 400 - {'error': {'message': 'litellm.BadRequestError: OpenAIException - max_tokens must be at least 1, got -144216.. Received Model Group=openai/gpt-oss-120b\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '400'}}

    During handling of the above exception, another exception occurred:

    [0;31mBadRequestError[0m                           Traceback (most recent call last)
    Cell [0;32mIn[7], line 2[0m
    [1;32m      1[0m q [38;5;241m=[39m [38;5;124m"[39m[38;5;124mWhat gifts does Agamemnon offer Achilles?[39m[38;5;124m"[39m
    [0;32m----> 2[0m [43mbenchmark_vanilla[49m[43m([49m[43mcontext[49m[43m,[49m[43m [49m
    [1;32m      3[0m [43m                  [49m[43mquery[49m[38;5;241;43m=[39;49m[43mq[49m[43m,[49m[43m [49m
    [1;32m      4[0m [43m                  [49m[43mmodel[49m[38;5;241;43m=[39;49m[38;5;124;43m"[39;49m[38;5;124;43mopenai/openai/gpt-oss-120b[39;49m[38;5;124;43m"[39;49m[43m,[49m[43m [49m
    [1;32m      5[0m [43m                  [49m[43mbase_url[49m[38;5;241;43m=[39;49m[38;5;124;43m"[39;49m[38;5;124;43mhttps://litellm-staging.gopay.sh[39;49m[38;5;124;43m"[39;49m
    [1;32m      6[0m [43m                 [49m[43m)[49m

    Cell [0;32mIn[3], line 16[0m, in [0;36mbenchmark_vanilla[0;34m(context, query, model, base_url)[0m
    [1;32m     13[0m [38;5;28;01mif[39;00m base_url:
    [1;32m     14[0m     kwargs[[38;5;124m"[39m[38;5;124mapi_base[39m[38;5;124m"[39m] [38;5;241m=[39m base_url
    [0;32m---> 16[0m response [38;5;241m=[39m [43mcompletion[49m[43m([49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
    [1;32m     17[0m elapsed [38;5;241m=[39m time[38;5;241m.[39mtime() [38;5;241m-[39m start_time
    [1;32m     19[0m [38;5;28;01mreturn[39;00m {
    [1;32m     20[0m     [38;5;124m"[39m[38;5;124mmethod[39m[38;5;124m"[39m: [38;5;124m"[39m[38;5;124mvanilla[39m[38;5;124m"[39m,
    [1;32m     21[0m     [38;5;124m"[39m[38;5;124mtime[39m[38;5;124m"[39m: elapsed,
    [1;32m     22[0m     [38;5;124m"[39m[38;5;124manswer[39m[38;5;124m"[39m: response[38;5;241m.[39mchoices[[38;5;241m0[39m][38;5;241m.[39mmessage[38;5;241m.[39mcontent,
    [1;32m     23[0m     [38;5;124m"[39m[38;5;124mtokens[39m[38;5;124m"[39m: response[38;5;241m.[39musage[38;5;241m.[39mtotal_tokens [38;5;28;01mif[39;00m [38;5;28mhasattr[39m(response, [38;5;124m'[39m[38;5;124musage[39m[38;5;124m'[39m) [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m
    [1;32m     24[0m }

    File [0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/utils.py:1381[0m, in [0;36mclient.<locals>.wrapper[0;34m(*args, **kwargs)[0m
    [1;32m   1377[0m [38;5;28;01mif[39;00m logging_obj:
    [1;32m   1378[0m     logging_obj[38;5;241m.[39mfailure_handler(
    [1;32m   1379[0m         e, traceback_exception, start_time, end_time
    [1;32m   1380[0m     )  [38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this![39;00m
    [0;32m-> 1381[0m [38;5;28;01mraise[39;00m e

    File [0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/utils.py:1250[0m, in [0;36mclient.<locals>.wrapper[0;34m(*args, **kwargs)[0m
    [1;32m   1248[0m         print_verbose([38;5;124mf[39m[38;5;124m"[39m[38;5;124mError while checking max token limit: [39m[38;5;132;01m{[39;00m[38;5;28mstr[39m(e)[38;5;132;01m}[39;00m[38;5;124m"[39m)
    [1;32m   1249[0m [38;5;66;03m# MODEL CALL[39;00m
    [0;32m-> 1250[0m result [38;5;241m=[39m [43moriginal_function[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
    [1;32m   1251[0m end_time [38;5;241m=[39m datetime[38;5;241m.[39mdatetime[38;5;241m.[39mnow()
    [1;32m   1252[0m [38;5;28;01mif[39;00m _is_streaming_request(
    [1;32m   1253[0m     kwargs[38;5;241m=[39mkwargs,
    [1;32m   1254[0m     call_type[38;5;241m=[39mcall_type,
    [1;32m   1255[0m ):

    File [0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/main.py:3772[0m, in [0;36mcompletion[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)[0m
    [1;32m   3769[0m     [38;5;28;01mreturn[39;00m response
    [1;32m   3770[0m [38;5;28;01mexcept[39;00m [38;5;167;01mException[39;00m [38;5;28;01mas[39;00m e:
    [1;32m   3771[0m     [38;5;66;03m## Map to OpenAI Exception[39;00m
    [0;32m-> 3772[0m     [38;5;28;01mraise[39;00m [43mexception_type[49m[43m([49m
    [1;32m   3773[0m [43m        [49m[43mmodel[49m[38;5;241;43m=[39;49m[43mmodel[49m[43m,[49m
    [1;32m   3774[0m [43m        [49m[43mcustom_llm_provider[49m[38;5;241;43m=[39;49m[43mcustom_llm_provider[49m[43m,[49m
    [1;32m   3775[0m [43m        [49m[43moriginal_exception[49m[38;5;241;43m=[39;49m[43me[49m[43m,[49m
    [1;32m   3776[0m [43m        [49m[43mcompletion_kwargs[49m[38;5;241;43m=[39;49m[43margs[49m[43m,[49m
    [1;32m   3777[0m [43m        [49m[43mextra_kwargs[49m[38;5;241;43m=[39;49m[43mkwargs[49m[43m,[49m
    [1;32m   3778[0m [43m    [49m[43m)[49m

    File [0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2328[0m, in [0;36mexception_type[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)[0m
    [1;32m   2326[0m [38;5;28;01mif[39;00m exception_mapping_worked:
    [1;32m   2327[0m     [38;5;28msetattr[39m(e, [38;5;124m"[39m[38;5;124mlitellm_response_headers[39m[38;5;124m"[39m, litellm_response_headers)
    [0;32m-> 2328[0m     [38;5;28;01mraise[39;00m e
    [1;32m   2329[0m [38;5;28;01melse[39;00m:
    [1;32m   2330[0m     [38;5;28;01mfor[39;00m error_type [38;5;129;01min[39;00m litellm[38;5;241m.[39mLITELLM_EXCEPTION_TYPES:

    File [0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:473[0m, in [0;36mexception_type[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)[0m
    [1;32m    471[0m [38;5;28;01mif[39;00m original_exception[38;5;241m.[39mstatus_code [38;5;241m==[39m [38;5;241m400[39m:
    [1;32m    472[0m     exception_mapping_worked [38;5;241m=[39m [38;5;28;01mTrue[39;00m
    [0;32m--> 473[0m     [38;5;28;01mraise[39;00m BadRequestError(
    [1;32m    474[0m         message[38;5;241m=[39m[38;5;124mf[39m[38;5;124m"[39m[38;5;132;01m{[39;00mexception_provider[38;5;132;01m}[39;00m[38;5;124m - [39m[38;5;132;01m{[39;00mmessage[38;5;132;01m}[39;00m[38;5;124m"[39m,
    [1;32m    475[0m         llm_provider[38;5;241m=[39mcustom_llm_provider,
    [1;32m    476[0m         model[38;5;241m=[39mmodel,
    [1;32m    477[0m         response[38;5;241m=[39m[38;5;28mgetattr[39m(original_exception, [38;5;124m"[39m[38;5;124mresponse[39m[38;5;124m"[39m, [38;5;28;01mNone[39;00m),
    [1;32m    478[0m         litellm_debug_info[38;5;241m=[39mextra_information,
    [1;32m    479[0m     )
    [1;32m    480[0m [38;5;28;01melif[39;00m original_exception[38;5;241m.[39mstatus_code [38;5;241m==[39m [38;5;241m401[39m:
    [1;32m    481[0m     exception_mapping_worked [38;5;241m=[39m [38;5;28;01mTrue[39;00m

    [0;31mBadRequestError[0m: litellm.BadRequestError: OpenAIException - litellm.BadRequestError: OpenAIException - max_tokens must be at least 1, got -144216.. Received Model Group=openai/gpt-oss-120b
    Available Model Group Fallbacks=None

``` python
benchmark_rlm(context, 
              query=q, 
              model="openai/openai/gpt-oss-120b", 
              base_url="https://<your_gateway>.com"
             )
```

    [RLM] Step: 1/100
      ‚Üí Tool: run_repl
    [RLM] Step: 2/100
      ‚Üí Tool: run_repl
    [RLM] Step: 3/100
      ‚Üí Tool: run_repl
    [RLM] Step: 4/100
      ‚Üí Tool: run_repl
    [RLM] Step: 5/100
      ‚Üí Tool: run_repl
    [RLM] Step: 6/100
      ‚Üí Tool: run_repl
    [RLM] Step: 7/100
      ‚Üí Tool: run_repl
    [RLM] Step: 8/100
      ‚Üí Tool: run_repl
    [RLM] Step: 9/100
      ‚Üí Tool: run_repl
    [RLM] Step: 10/100
      ‚Üí Tool: run_repl
    [RLM] Step: 11/100
      ‚Üí Tool: run_repl
    [RLM] Step: 12/100
    [RLM] Using fallback: no FINAL() detected

    {'method': 'rlm',
     'time': 55.75939989089966,
     'answer': 'In the embassy to\u202fAchilles (the ‚ÄúEmbassy to Achilles‚Äù in Book\u202fIX), Agamemnon tries to win the hero back by promising a very large lump‚Äësum of wealth and a host of valuable prizes.  In his speech he enumerates the gifts as follows:\n\n* **Ten weighty talents of the purest gold** ‚Äì a massive amount of gold.  \n* **Twenty fine vases (twice ten) of shining, re‚Äëfulgent metal** ‚Äì ceremonial vessels.  \n* **Seven sacred golden tripods** ‚Äì the prized votive‚Äëtripods that were offered to the gods.  \n* **Twelve swift, unmatched steeds** ‚Äì the finest war‚Äëhorses.  \n* **Seven beautiful captive women from the Lesbian line** ‚Äì a group of desirable concubines/hostesses.  \n* **(And) the return of Briseis** ‚Äì the very woman whose seizure had provoked Achilles‚Äô wrath.\n\nThus, Agamemnon offers Achilles a combination of gold, precious vessels, sacred tripods, fast horses, a set of beautiful captive women, and the restitution of Briseis as the ‚Äúgift‚Äëpackage‚Äù intended to appease him.',
     'tokens': 66891}
