{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "\n",
    "from lisette import *\n",
    "from toolslm.funccall import mk_ns, call_func\n",
    "from litellm import completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def advanced_toolloop(message, sp, tools, sh=None, model='gpt-4', base_url=None, max_steps=10, \n",
    "                      final_prompt=None,\n",
    "                      verbose=False\n",
    "                     ):\n",
    "    \"\"\"\n",
    "    Advanced tool loop for RLM that handles REPL code execution and recursive LLM calls.\n",
    "    \n",
    "    Args:\n",
    "        message: User query to answer\n",
    "        sp: System prompt for the root LLM\n",
    "        tools: List of tool functions (typically [run_repl])\n",
    "        sh: Shell instance for FINAL_VAR variable lookup\n",
    "        model: Model name\n",
    "        base_url: API base URL\n",
    "        max_steps: Maximum iteration steps\n",
    "        final_prompt: Prompt to add if max_steps reached without FINAL\n",
    "    Yields:\n",
    "        LLM responses, tool results, and final answer dict\n",
    "    \"\"\"\n",
    "    if final_prompt is None:\n",
    "        final_prompt = \"Summarize your findings based on the tool results.\"\n",
    "    \n",
    "    ns = mk_ns(tools)\n",
    "    tool_schemas = [lite_mk_func(t) for t in tools]\n",
    "\n",
    "    if sp is not None:\n",
    "        messages = [{'role': 'system', 'content': sp}, \n",
    "                    {'role': 'user', 'content': message}]\n",
    "    else:\n",
    "        messages = [{'role': 'user', 'content': message}]\n",
    "    \n",
    "    final_found = False\n",
    "        \n",
    "    for step in range(1, max_steps + 1):\n",
    "        if verbose:\n",
    "            print(f\"[RLM] Step: {step}/{max_steps}\")\n",
    "            \n",
    "        is_final = (step == max_steps)\n",
    "        tool_choice = 'none' if is_final else None\n",
    "        \n",
    "        kwargs = {\"model\": model, \"messages\": messages, \"tools\": tool_schemas, \"tool_choice\": tool_choice}\n",
    "        if base_url:\n",
    "            kwargs[\"api_base\"] = base_url\n",
    "        \n",
    "        r = completion(**kwargs)\n",
    "        yield r\n",
    "        \n",
    "        msg = r.choices[0].message\n",
    "        messages.append(msg)\n",
    "\n",
    "        if msg.content:\n",
    "            final_match = re.search(r'FINAL\\((.*?)\\)', msg.content, re.DOTALL)\n",
    "            final_var_match = re.search(r'FINAL_VAR\\(([^)]+)\\)', msg.content)\n",
    "    \n",
    "            if final_match:\n",
    "                if verbose:\n",
    "                    print(f\"[RLM] FINAL answer found\")\n",
    "                \n",
    "                answer = final_match.group(1).strip()\n",
    "                yield {\"type\": \"final\", \"answer\": answer}\n",
    "                final_found = True\n",
    "                break\n",
    "            elif final_var_match:\n",
    "                if verbose:\n",
    "                    print(f\"[RLM] FINAL answer found\")\n",
    "                \n",
    "                var_name = final_var_match.group(1).strip()\n",
    "                answer = sh.user_ns.get(var_name, f\"Variable {var_name} not found\")\n",
    "                yield {\"type\": \"final\", \"answer\": str(answer)}\n",
    "                final_found = True\n",
    "                break\n",
    "\n",
    "        if tool_calls := msg.tool_calls:\n",
    "            for tc in tool_calls:\n",
    "                if verbose:\n",
    "                    print(f\"  â†’ Tool: {tc.function.name}\")\n",
    "                \n",
    "                args = json.loads(tc.function.arguments)\n",
    "                result = call_func(tc.function.name, args, ns=ns)\n",
    "                tool_result = {\n",
    "                    \"tool_call_id\": tc.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": tc.function.name,\n",
    "                    \"content\": str(result)\n",
    "                }\n",
    "                messages.append(tool_result)\n",
    "                yield tool_result\n",
    "            \n",
    "            if is_final:\n",
    "                messages.append({'role': 'user', 'content': final_prompt})\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Fallback: if no FINAL() was found, use last assistant message\n",
    "    if not final_found:\n",
    "        if verbose:\n",
    "            print(f\"[RLM] Using fallback: no FINAL() detected\")\n",
    "        for msg in reversed(messages):\n",
    "            if isinstance(msg, dict) and msg.get('role') == 'assistant':\n",
    "                yield {\"type\": \"final\", \"answer\": msg.get('content', 'No answer provided')}\n",
    "                break\n",
    "            elif hasattr(msg, 'content') and msg.content:\n",
    "                yield {\"type\": \"final\", \"answer\": msg.content}\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlm",
   "language": "python",
   "name": "rlm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
