{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tools\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "import os, re\n",
    "import json\n",
    "\n",
    "from lisette import *\n",
    "\n",
    "from toolslm.funccall import mk_ns, call_func\n",
    "from toolslm.shell import get_shell\n",
    "\n",
    "from litellm import completion\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prep_shell(context, model=\"openai/openai/gpt-oss-120b\", base_url=None):\n",
    "    \"\"\"\n",
    "    Prepare a REPL shell environment for RLM with context and recursive LLM query capability.\n",
    "    \n",
    "    Args:\n",
    "        context: The text/data to be analyzed (stored as a variable in the shell)\n",
    "        model: Model name (OpenAI-compatible)\n",
    "        base_url: API base URL for your LLM gateway\n",
    "    \n",
    "    Returns:\n",
    "        IPython shell instance with `context` and `llm_query()` available\n",
    "    \"\"\"\n",
    "    def llm_query(query: str):\n",
    "        \"Call an LLM with the given query and return the answer\"\n",
    "        kwargs = {\"model\": model, \"messages\": [{\"role\": \"user\", \"content\": query}]}\n",
    "        if base_url:\n",
    "            kwargs[\"api_base\"] = base_url\n",
    "        response = completion(**kwargs)\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    sh = get_shell()\n",
    "    sh.user_ns['context'] = context\n",
    "    sh.user_ns['llm_query'] = llm_query\n",
    "    return sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_run_repl(sh, max_output=5000):\n",
    "    \"\"\"\n",
    "    Create a run_repl tool function that executes Python code in the given shell.\n",
    "    \n",
    "    Args:\n",
    "        sh: IPython shell instance (from prep_shell)\n",
    "        max_output: Maximum characters to return from output (default: 5000)\n",
    "    \n",
    "    Returns:\n",
    "        Function that executes code and returns output (truncated to max_output chars)\n",
    "    \"\"\"\n",
    "    def run_repl(python_code: str) -> str:\n",
    "        \"Execute Python code in the REPL environment\"\n",
    "        result = sh.run_cell(python_code)\n",
    "        if result.error_in_exec or result.error_before_exec:\n",
    "            return f\"Error: {result.error_in_exec or result.error_before_exec}\"\n",
    "        output = result.stdout or \"\"\n",
    "        if result.result is not None:\n",
    "            output += str(result.result)\n",
    "        return output[:max_output] if max_output else output\n",
    "    return run_repl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
