{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from rlm.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLM - Recursive Language Models\n",
    "\n",
    "> Enable language models to process unbounded context through recursive self-interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "Modern LLMs suffer from **\"context rot\"** - as context grows, performance degrades even when it fits within the model's window. A 100k token conversation causes the model to \"forget\" or give lower-quality responses, despite technically being able to process it all.\n",
    "\n",
    "Traditional solutions:\n",
    "- **Bigger context windows** → Still suffer from rot, expensive\n",
    "- **RAG/retrieval** → Requires pre-indexing, rigid search strategies\n",
    "\n",
    "## The Solution\n",
    "\n",
    "**Recursive Language Models (RLMs)** treat context as a programmable object that models explore adaptively at test-time. Instead of cramming everything into one call, the model recursively breaks down and processes context in a REPL environment.\n",
    "\n",
    "**Key result from the paper:** RLM using GPT-4-mini outperforms vanilla GPT-4 by 2x on long-context benchmarks while costing the same or less!\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **Context as variable** - Store your document in a Python REPL environment\n",
    "2. **Adaptive exploration** - Root LM decides how to chunk, search, and process\n",
    "3. **Recursive queries** - Call `llm_query()` on manageable chunks\n",
    "4. **No context rot** - Each model call works with small, focused context\n",
    "\n",
    "Based on the paper: [Recursive Language Models](https://alexzhang13.github.io/blog/2025/rlm/) by Alex Zhang et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developer Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are new to using `nbdev` here are some useful pointers to get you started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install rlm in Development mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "# make sure rlm package is installed in development mode\n",
    "$ pip install -e .\n",
    "\n",
    "# make changes under nbs/ directory\n",
    "# ...\n",
    "\n",
    "# compile to have changes apply to rlm\n",
    "$ nbdev_prepare\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install latest from the GitHub [repository][repo]:\n",
    "\n",
    "```sh\n",
    "$ pip install git+https://github.com/numb3r33/rlm.git\n",
    "```\n",
    "\n",
    "or from [conda][conda]\n",
    "\n",
    "```sh\n",
    "$ conda install -c numb3r33 rlm\n",
    "```\n",
    "\n",
    "or from [pypi][pypi]\n",
    "\n",
    "\n",
    "```sh\n",
    "$ pip install rlm\n",
    "```\n",
    "\n",
    "\n",
    "[repo]: https://github.com/numb3r33/rlm\n",
    "[docs]: https://numb3r33.github.io/rlm/\n",
    "[pypi]: https://pypi.org/project/rlm/\n",
    "[conda]: https://anaconda.org/numb3r33/rlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation can be found hosted on this GitHub [repository][repo]'s [pages][docs]. Additionally you can find package manager specific guidelines on [conda][conda] and [pypi][pypi] respectively.\n",
    "\n",
    "[repo]: https://github.com/numb3r33/rlm\n",
    "[docs]: https://numb3r33.github.io/rlm/\n",
    "[pypi]: https://pypi.org/project/rlm/\n",
    "[conda]: https://anaconda.org/numb3r33/rlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "\n",
    "Here's a simple example of using RLM to answer questions over long documents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export OPENAI_API_KEY=\"your-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from rlm.tools import prep_shell, make_run_repl \n",
    "from rlm.core import advanced_toolloop \n",
    "from rlm.prompts import REPL_SYSTEM_PROMPT\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your long document/context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "with open(\"document.txt\") as f: \n",
    "    context = f.read()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup RLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sh = prep_shell(context, model=\"openai/openai/gpt-oss-120b\", base_url=\"https://your-litellm-gateway.com\")\n",
    "run_repl = make_run_repl(sh)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask a question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query = \"What are the main themes discussed in this document?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run RLM with verbose output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "responses = advanced_toolloop( query, sp=REPL_SYSTEM_PROMPT, tools=[run_repl], sh=sh, model=\"openai/openai/gpt-oss-120b\", base_url=\"https://your-litellm-gateway.com\", max_steps=50, verbose=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for item in responses:\n",
    "    if isinstance(item, dict) and item.get(\"type\") == \"final\":\n",
    "        print(f\"Answer: {item['answer']}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanialla approach ( may fail with very long context )\n",
    "\n",
    "```\n",
    "try:\n",
    "    vanilla_result = benchmark_vanilla(context, query, model=\"gpt-4\", base_url=\"...\")\n",
    "    print(f\"Vanilla: {vanilla_result['time']:.2f}s, {vanilla_result['tokens']} tokens\")\n",
    "except Exception as e:\n",
    "    print(f\"Vanilla failed: {e}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RLM approach\n",
    "\n",
    "```\n",
    "rlm_result = benchmark_rlm(context, query, model=\"gpt-4\", base_url=\"...\", verbose=True)\n",
    "print(f\"RLM: {rlm_result['time']:.2f}s, {rlm_result['tokens']} tokens\")\n",
    "print(f\"Answer: {rlm_result['answer']}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAQ\n",
    "\n",
    "### When should I use RLM vs vanilla LLM?\n",
    "\n",
    "**Use RLM when:**\n",
    "- Context exceeds model's window (100k+ tokens)\n",
    "- You experience \"context rot\" (model gets worse with long conversations)\n",
    "- Task requires reasoning across many documents\n",
    "- You want adaptive, test-time chunking strategies\n",
    "\n",
    "**Use vanilla when:**\n",
    "- Context is short (< 10k tokens)\n",
    "- Simple fact retrieval\n",
    "- Speed is critical and context fits easily\n",
    "\n",
    "### What's the difference between `max_steps` and recursion depth?\n",
    "\n",
    "- **`max_steps`**: How many REPL iterations the root LM gets (horizontal - loop count)\n",
    "- **Recursion depth**: How deep calls can nest (vertical - call stack depth)\n",
    "\n",
    "RLM enforces depth=1 by design: root LM can call `llm_query()`, but those calls can't spawn further recursion.\n",
    "\n",
    "### Why doesn't the model always use FINAL()?\n",
    "\n",
    "Some models don't consistently follow the FINAL() instruction. RLM includes a fallback that captures the last assistant message if FINAL() isn't detected.\n",
    "\n",
    "### How does RLM compare to RAG?\n",
    "\n",
    "**RLM advantages:**\n",
    "- No pre-indexing needed\n",
    "- Adaptive search strategies (model decides how to explore)\n",
    "- Better for complex multi-step reasoning\n",
    "\n",
    "**RAG advantages:**\n",
    "- Faster for simple lookups\n",
    "- Works well with persistent knowledge bases\n",
    "- Lower cost per query for repeated queries\n",
    "\n",
    "### Can I customize the system prompt?\n",
    "\n",
    "Yes! Import and modify `REPL_SYSTEM_PROMPT` or create your own:\n",
    "\n",
    "```\n",
    "from rlm.prompts import REPL_SYSTEM_PROMPT\n",
    "\n",
    "custom_prompt = REPL_SYSTEM_PROMPT + \"\\nAdditional instructions here...\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Directions\n",
    "\n",
    "Based on the [Recursive Language Models paper](https://alexzhang13.github.io/blog/2025/rlm/), here are planned enhancements:\n",
    "\n",
    "### Short-term\n",
    "- **Token/cost tracking**: Detailed metrics for each step\n",
    "- **Multiple benchmark tasks**: Expand beyond document Q&A\n",
    "- **Error recovery improvements**: Better handling of API failures and malformed tool calls\n",
    "- **Configurable FINAL detection**: Custom patterns beyond FINAL() and FINAL_VAR()\n",
    "\n",
    "### Medium-term\n",
    "- **Training for recursion**: Fine-tune models explicitly for RLM patterns (like o1 for reasoning)\n",
    "- **Deeper recursion**: Support depth > 1 for more complex tasks\n",
    "- **Multi-modal context**: Support for images, tables, structured data\n",
    "- **Streaming responses**: Real-time answer updates as RLM progresses\n",
    "\n",
    "### Long-term\n",
    "- **RL-based optimization**: Learn optimal chunking and recursion strategies\n",
    "- **Hybrid RAG+RLM**: Combine pre-indexed retrieval with adaptive exploration\n",
    "- **Benchmark suite**: Comprehensive evaluation across domains\n",
    "\n",
    "## Contributing\n",
    "\n",
    "We welcome contributions! Areas where help is needed:\n",
    "- Additional benchmark tasks\n",
    "- Prompt engineering for better FINAL() compliance\n",
    "- Performance optimizations\n",
    "- Documentation improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
