# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_benchmark.ipynb.

# %% auto 0
__all__ = ['benchmark_vanilla', 'benchmark_rlm']

# %% ../nbs/02_benchmark.ipynb 1
import time
from litellm import completion
from .tools import prep_shell, make_run_repl
from .core import advanced_toolloop
from .prompts import REPL_SYSTEM_PROMPT

# %% ../nbs/02_benchmark.ipynb 2
def benchmark_vanilla(context, query, model="gpt-4", base_url=None):
    """
    Benchmark vanilla LLM approach (direct call with full context).
    
    Returns dict with: method, time, answer, tokens (if available)
    """
    start_time = time.time()
    kwargs = {
        "model": model,
        "messages": [{"role": "user", "content": f"Context: {context}\n\nQuestion: {query}"}]
    }
    if base_url:
        kwargs["api_base"] = base_url
    
    response = completion(**kwargs)
    elapsed = time.time() - start_time
    
    return {
        "method": "vanilla",
        "time": elapsed,
        "answer": response.choices[0].message.content,
        "tokens": response.usage.total_tokens if hasattr(response, 'usage') else None
    }

# %% ../nbs/02_benchmark.ipynb 3
def benchmark_rlm(context, query, model="gpt-4", base_url=None, max_steps=100):
    """
    Benchmark RLM approach with REPL environment and recursive calls.
    
    Returns dict with: method, time, answer, tokens (if available)
    """
    start_time = time.time()
    sh = prep_shell(context, model=model, base_url=base_url)
    run_repl = make_run_repl(sh)
    
    outer_chat_response = advanced_toolloop(
        query, 
        sp=REPL_SYSTEM_PROMPT, 
        tools=[run_repl], 
        sh=sh, 
        model=model,
        base_url=base_url,
        max_steps=max_steps,
        verbose=True
    )
    
    final_answer = None
    total_tokens = 0
    
    for item in outer_chat_response:
        if isinstance(item, dict) and item.get("type") == "final":
            final_answer = item['answer']
        elif hasattr(item, 'usage'):
            total_tokens += item.usage.total_tokens
    
    elapsed = time.time() - start_time
    
    return {
        "method": "rlm",
        "time": elapsed,
        "answer": final_answer,
        "tokens": total_tokens if total_tokens > 0 else None
    }
