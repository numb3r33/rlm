[
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nadvanced_toolloop\n\n advanced_toolloop (message, sp, tools, sh=None, model='gpt-4',\n                    base_url=None, max_steps=10, final_prompt=None,\n                    verbose=False)\n\n*Advanced tool loop for RLM that handles REPL code execution and recursive LLM calls.\nArgs: message: User query to answer sp: System prompt for the root LLM tools: List of tool functions (typically [run_repl]) sh: Shell instance for FINAL_VAR variable lookup model: Model name base_url: API base URL max_steps: Maximum iteration steps final_prompt: Prompt to add if max_steps reached without FINAL Yields: LLM responses, tool results, and final answer dict*",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "02_benchmark.html",
    "href": "02_benchmark.html",
    "title": "rlm",
    "section": "",
    "text": "source\n\nbenchmark_vanilla\n\n benchmark_vanilla (context, query, model='gpt-4', base_url=None)\n\n*Benchmark vanilla LLM approach (direct call with full context).\nReturns dict with: method, time, answer, tokens (if available)*\n\nsource\n\n\nbenchmark_rlm\n\n benchmark_rlm (context, query, model='gpt-4', base_url=None,\n                max_steps=100)\n\n*Benchmark RLM approach with REPL environment and recursive calls.\nReturns dict with: method, time, answer, tokens (if available)*\n\nwith open('../context/illiad.txt', 'r') as f:\n    context = f.read()\n\n\nq = \"What gifts does Agamemnon offer Achilles?\"\n\n\nbenchmark_vanilla(context, \n                  query=q, \n                  model=\"openai/openai/gpt-oss-120b\", \n                  base_url=\"https://&lt;your_gateway&gt;.com\"\n                 )\n\n\nGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\n\n\n\n---------------------------------------------------------------------------\nBadRequestError                           Traceback (most recent call last)\nFile /opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/llms/openai/openai.py:745, in OpenAIChatCompletion.completion(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params, shared_session)\n    744             else:\n--&gt; 745                 raise e\n    746 except OpenAIError as e:\n\nFile /opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/llms/openai/openai.py:673, in OpenAIChatCompletion.completion(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params, shared_session)\n    659 logging_obj.pre_call(\n    660     input=messages,\n    661     api_key=openai_client.api_key,\n   (...)\n    667     },\n    668 )\n    670 (\n    671     headers,\n    672     response,\n--&gt; 673 ) = self.make_sync_openai_chat_completion_request(\n    674     openai_client=openai_client,\n    675     data=data,\n    676     timeout=timeout,\n    677     logging_obj=logging_obj,\n    678 )\n    680 logging_obj.model_call_details[\"response_headers\"] = headers\n\nFile /opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:237, in track_llm_api_timing.&lt;locals&gt;.decorator.&lt;locals&gt;.sync_wrapper(*args, **kwargs)\n    236 try:\n--&gt; 237     result = func(*args, **kwargs)\n    238     return result\n\nFile /opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/llms/openai/openai.py:489, in OpenAIChatCompletion.make_sync_openai_chat_completion_request(self, openai_client, data, timeout, logging_obj)\n    488 else:\n--&gt; 489     raise e\n\nFile /opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/llms/openai/openai.py:471, in OpenAIChatCompletion.make_sync_openai_chat_completion_request(self, openai_client, data, timeout, logging_obj)\n    470 try:\n--&gt; 471     raw_response = openai_client.chat.completions.with_raw_response.create(\n    472         **data, timeout=timeout\n    473     )\n    475     if hasattr(raw_response, \"headers\"):\n\nFile /opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/openai/_legacy_response.py:364, in to_raw_response_wrapper.&lt;locals&gt;.wrapped(*args, **kwargs)\n    362 kwargs[\"extra_headers\"] = extra_headers\n--&gt; 364 return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n\nFile /opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/openai/_utils/_utils.py:286, in required_args.&lt;locals&gt;.inner.&lt;locals&gt;.wrapper(*args, **kwargs)\n    285     raise TypeError(msg)\n--&gt; 286 return func(*args, **kwargs)\n\nFile /opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1189, in Completions.create(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\n   1188 validate_response_format(response_format)\n-&gt; 1189 return self._post(\n   1190     \"/chat/completions\",\n   1191     body=maybe_transform(\n   1192         {\n   1193             \"messages\": messages,\n   1194             \"model\": model,\n   1195             \"audio\": audio,\n   1196             \"frequency_penalty\": frequency_penalty,\n   1197             \"function_call\": function_call,\n   1198             \"functions\": functions,\n   1199             \"logit_bias\": logit_bias,\n   1200             \"logprobs\": logprobs,\n   1201             \"max_completion_tokens\": max_completion_tokens,\n   1202             \"max_tokens\": max_tokens,\n   1203             \"metadata\": metadata,\n   1204             \"modalities\": modalities,\n   1205             \"n\": n,\n   1206             \"parallel_tool_calls\": parallel_tool_calls,\n   1207             \"prediction\": prediction,\n   1208             \"presence_penalty\": presence_penalty,\n   1209             \"prompt_cache_key\": prompt_cache_key,\n   1210             \"prompt_cache_retention\": prompt_cache_retention,\n   1211             \"reasoning_effort\": reasoning_effort,\n   1212             \"response_format\": response_format,\n   1213             \"safety_identifier\": safety_identifier,\n   1214             \"seed\": seed,\n   1215             \"service_tier\": service_tier,\n   1216             \"stop\": stop,\n   1217             \"store\": store,\n   1218             \"stream\": stream,\n   1219             \"stream_options\": stream_options,\n   1220             \"temperature\": temperature,\n   1221             \"tool_choice\": tool_choice,\n   1222             \"tools\": tools,\n   1223             \"top_logprobs\": top_logprobs,\n   1224             \"top_p\": top_p,\n   1225             \"user\": user,\n   1226             \"verbosity\": verbosity,\n   1227             \"web_search_options\": web_search_options,\n   1228         },\n   1229         completion_create_params.CompletionCreateParamsStreaming\n   1230         if stream\n   1231         else completion_create_params.CompletionCreateParamsNonStreaming,\n   1232     ),\n   1233     options=make_request_options(\n   1234         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n   1235     ),\n   1236     cast_to=ChatCompletion,\n   1237     stream=stream or False,\n   1238     stream_cls=Stream[ChatCompletionChunk],\n   1239 )\n\nFile /opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/openai/_base_client.py:1259, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)\n   1256 opts = FinalRequestOptions.construct(\n   1257     method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1258 )\n-&gt; 1259 return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n\nFile /opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/openai/_base_client.py:1047, in SyncAPIClient.request(self, cast_to, options, stream, stream_cls)\n   1046     log.debug(\"Re-raising status error\")\n-&gt; 1047     raise self._make_status_error_from_response(err.response) from None\n   1049 break\n\nBadRequestError: Error code: 400 - {'error': {'message': 'litellm.BadRequestError: OpenAIException - max_tokens must be at least 1, got -144216.. Received Model Group=openai/gpt-oss-120b\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '400'}}\n\nDuring handling of the above exception, another exception occurred:\n\nOpenAIError                               Traceback (most recent call last)\nFile /opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/main.py:2158, in completion(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\n   2152     logging.post_call(\n   2153         input=messages,\n   2154         api_key=api_key,\n   2155         original_response=str(e),\n   2156         additional_args={\"headers\": headers},\n   2157     )\n-&gt; 2158     raise e\n   2160 if optional_params.get(\"stream\", False):\n   2161     ## LOGGING\n\nFile /opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/main.py:2130, in completion(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\n   2129     else:\n-&gt; 2130         response = openai_chat_completions.completion(\n   2131             model=model,\n   2132             messages=messages,\n   2133             headers=headers,\n   2134             model_response=model_response,\n   2135             print_verbose=print_verbose,\n   2136             api_key=api_key,\n   2137             api_base=api_base,\n   2138             acompletion=acompletion,\n   2139             logging_obj=logging,\n   2140             optional_params=optional_params,\n   2141             litellm_params=litellm_params,\n   2142             logger_fn=logger_fn,\n   2143             timeout=timeout,  # type: ignore\n   2144             custom_prompt_dict=custom_prompt_dict,\n   2145             client=client,  # pass AsyncOpenAI, OpenAI client\n   2146             organization=organization,\n   2147             custom_llm_provider=custom_llm_provider,\n   2148             shared_session=shared_session,\n   2149         )\n   2150 except Exception as e:\n   2151     ## LOGGING - log the original exception returned\n\nFile /opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/llms/openai/openai.py:756, in OpenAIChatCompletion.completion(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params, shared_session)\n    755     error_headers = getattr(error_response, \"headers\", None)\n--&gt; 756 raise OpenAIError(\n    757     status_code=status_code,\n    758     message=error_text,\n    759     headers=error_headers,\n    760     body=error_body,\n    761 )\n\nOpenAIError: Error code: 400 - {'error': {'message': 'litellm.BadRequestError: OpenAIException - max_tokens must be at least 1, got -144216.. Received Model Group=openai/gpt-oss-120b\\nAvailable Model Group Fallbacks=None', 'type': None, 'param': None, 'code': '400'}}\n\nDuring handling of the above exception, another exception occurred:\n\nBadRequestError                           Traceback (most recent call last)\nCell In[7], line 2\n      1 q = \"What gifts does Agamemnon offer Achilles?\"\n----&gt; 2 benchmark_vanilla(context, \n      3                   query=q, \n      4                   model=\"openai/openai/gpt-oss-120b\", \n      5                   base_url=\"https://litellm-staging.gopay.sh\"\n      6                  )\n\nCell In[3], line 16, in benchmark_vanilla(context, query, model, base_url)\n     13 if base_url:\n     14     kwargs[\"api_base\"] = base_url\n---&gt; 16 response = completion(**kwargs)\n     17 elapsed = time.time() - start_time\n     19 return {\n     20     \"method\": \"vanilla\",\n     21     \"time\": elapsed,\n     22     \"answer\": response.choices[0].message.content,\n     23     \"tokens\": response.usage.total_tokens if hasattr(response, 'usage') else None\n     24 }\n\nFile /opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/utils.py:1381, in client.&lt;locals&gt;.wrapper(*args, **kwargs)\n   1377 if logging_obj:\n   1378     logging_obj.failure_handler(\n   1379         e, traceback_exception, start_time, end_time\n   1380     )  # DO NOT MAKE THREADED - router retry fallback relies on this!\n-&gt; 1381 raise e\n\nFile /opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/utils.py:1250, in client.&lt;locals&gt;.wrapper(*args, **kwargs)\n   1248         print_verbose(f\"Error while checking max token limit: {str(e)}\")\n   1249 # MODEL CALL\n-&gt; 1250 result = original_function(*args, **kwargs)\n   1251 end_time = datetime.datetime.now()\n   1252 if _is_streaming_request(\n   1253     kwargs=kwargs,\n   1254     call_type=call_type,\n   1255 ):\n\nFile /opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/main.py:3772, in completion(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\n   3769     return response\n   3770 except Exception as e:\n   3771     ## Map to OpenAI Exception\n-&gt; 3772     raise exception_type(\n   3773         model=model,\n   3774         custom_llm_provider=custom_llm_provider,\n   3775         original_exception=e,\n   3776         completion_kwargs=args,\n   3777         extra_kwargs=kwargs,\n   3778     )\n\nFile /opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2328, in exception_type(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\n   2326 if exception_mapping_worked:\n   2327     setattr(e, \"litellm_response_headers\", litellm_response_headers)\n-&gt; 2328     raise e\n   2329 else:\n   2330     for error_type in litellm.LITELLM_EXCEPTION_TYPES:\n\nFile /opt/homebrew/Caskroom/miniconda/base/envs/llm_env/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:473, in exception_type(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\n    471 if original_exception.status_code == 400:\n    472     exception_mapping_worked = True\n--&gt; 473     raise BadRequestError(\n    474         message=f\"{exception_provider} - {message}\",\n    475         llm_provider=custom_llm_provider,\n    476         model=model,\n    477         response=getattr(original_exception, \"response\", None),\n    478         litellm_debug_info=extra_information,\n    479     )\n    480 elif original_exception.status_code == 401:\n    481     exception_mapping_worked = True\n\nBadRequestError: litellm.BadRequestError: OpenAIException - litellm.BadRequestError: OpenAIException - max_tokens must be at least 1, got -144216.. Received Model Group=openai/gpt-oss-120b\nAvailable Model Group Fallbacks=None\n\n\n\n\nbenchmark_rlm(context, \n              query=q, \n              model=\"openai/openai/gpt-oss-120b\", \n              base_url=\"https://&lt;your_gateway&gt;.com\"\n             )\n\n[RLM] Step: 1/100\n  → Tool: run_repl\n[RLM] Step: 2/100\n  → Tool: run_repl\n[RLM] Step: 3/100\n  → Tool: run_repl\n[RLM] Step: 4/100\n  → Tool: run_repl\n[RLM] Step: 5/100\n  → Tool: run_repl\n[RLM] Step: 6/100\n  → Tool: run_repl\n[RLM] Step: 7/100\n  → Tool: run_repl\n[RLM] Step: 8/100\n  → Tool: run_repl\n[RLM] Step: 9/100\n  → Tool: run_repl\n[RLM] Step: 10/100\n  → Tool: run_repl\n[RLM] Step: 11/100\n  → Tool: run_repl\n[RLM] Step: 12/100\n[RLM] Using fallback: no FINAL() detected\n\n\n{'method': 'rlm',\n 'time': 55.75939989089966,\n 'answer': 'In the embassy to\\u202fAchilles (the “Embassy to Achilles” in Book\\u202fIX), Agamemnon tries to win the hero back by promising a very large lump‑sum of wealth and a host of valuable prizes.  In his speech he enumerates the gifts as follows:\\n\\n* **Ten weighty talents of the purest gold** – a massive amount of gold.  \\n* **Twenty fine vases (twice ten) of shining, re‑fulgent metal** – ceremonial vessels.  \\n* **Seven sacred golden tripods** – the prized votive‑tripods that were offered to the gods.  \\n* **Twelve swift, unmatched steeds** – the finest war‑horses.  \\n* **Seven beautiful captive women from the Lesbian line** – a group of desirable concubines/hostesses.  \\n* **(And) the return of Briseis** – the very woman whose seizure had provoked Achilles’ wrath.\\n\\nThus, Agamemnon offers Achilles a combination of gold, precious vessels, sacred tripods, fast horses, a set of beautiful captive women, and the restitution of Briseis as the “gift‑package” intended to appease him.',\n 'tokens': 66891}",
    "crumbs": [
      "02_benchmark.html"
    ]
  },
  {
    "objectID": "tools.html",
    "href": "tools.html",
    "title": "tools",
    "section": "",
    "text": "source\n\nprep_shell\n\n prep_shell (context, model='openai/openai/gpt-oss-120b', base_url=None)\n\n*Prepare a REPL shell environment for RLM with context and recursive LLM query capability.\nArgs: context: The text/data to be analyzed (stored as a variable in the shell) model: Model name (OpenAI-compatible) base_url: API base URL for your LLM gateway\nReturns: IPython shell instance with context and llm_query() available*\n\nsource\n\n\nmake_run_repl\n\n make_run_repl (sh, max_output=5000)\n\n*Create a run_repl tool function that executes Python code in the given shell.\nArgs: sh: IPython shell instance (from prep_shell) max_output: Maximum characters to return from output (default: 5000)\nReturns: Function that executes code and returns output (truncated to max_output chars)*",
    "crumbs": [
      "tools"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RLM - Recursive Language Models",
    "section": "",
    "text": "Modern LLMs suffer from “context rot” - as context grows, performance degrades even when it fits within the model’s window. A 100k token conversation causes the model to “forget” or give lower-quality responses, despite technically being able to process it all.\nTraditional solutions: - Bigger context windows → Still suffer from rot, expensive - RAG/retrieval → Requires pre-indexing, rigid search strategies",
    "crumbs": [
      "RLM - Recursive Language Models"
    ]
  },
  {
    "objectID": "index.html#the-problem",
    "href": "index.html#the-problem",
    "title": "RLM - Recursive Language Models",
    "section": "",
    "text": "Modern LLMs suffer from “context rot” - as context grows, performance degrades even when it fits within the model’s window. A 100k token conversation causes the model to “forget” or give lower-quality responses, despite technically being able to process it all.\nTraditional solutions: - Bigger context windows → Still suffer from rot, expensive - RAG/retrieval → Requires pre-indexing, rigid search strategies",
    "crumbs": [
      "RLM - Recursive Language Models"
    ]
  },
  {
    "objectID": "index.html#the-solution",
    "href": "index.html#the-solution",
    "title": "RLM - Recursive Language Models",
    "section": "The Solution",
    "text": "The Solution\nRecursive Language Models (RLMs) treat context as a programmable object that models explore adaptively at test-time. Instead of cramming everything into one call, the model recursively breaks down and processes context in a REPL environment.\nKey result from the paper: RLM using GPT-4-mini outperforms vanilla GPT-4 by 2x on long-context benchmarks while costing the same or less!",
    "crumbs": [
      "RLM - Recursive Language Models"
    ]
  },
  {
    "objectID": "index.html#how-it-works",
    "href": "index.html#how-it-works",
    "title": "RLM - Recursive Language Models",
    "section": "How It Works",
    "text": "How It Works\n\nContext as variable - Store your document in a Python REPL environment\nAdaptive exploration - Root LM decides how to chunk, search, and process\nRecursive queries - Call llm_query() on manageable chunks\nNo context rot - Each model call works with small, focused context\n\nBased on the paper: Recursive Language Models by Alex Zhang et al.",
    "crumbs": [
      "RLM - Recursive Language Models"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "RLM - Recursive Language Models",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall rlm in Development mode\n# make sure rlm package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to rlm\n$ nbdev_prepare",
    "crumbs": [
      "RLM - Recursive Language Models"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "RLM - Recursive Language Models",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/numb3r33/rlm.git\nor from conda\n$ conda install -c numb3r33 rlm\nor from pypi\n$ pip install rlm\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages. Additionally you can find package manager specific guidelines on conda and pypi respectively.",
    "crumbs": [
      "RLM - Recursive Language Models"
    ]
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "RLM - Recursive Language Models",
    "section": "Quick Start",
    "text": "Quick Start\nHere’s a simple example of using RLM to answer questions over long documents:\nexport OPENAI_API_KEY=“your-key-here”\n\nBasic Usage\nfrom rlm.tools import prep_shell, make_run_repl \nfrom rlm.core import advanced_toolloop \nfrom rlm.prompts import REPL_SYSTEM_PROMPT\n\n\nYour long document/context\nwith open(\"document.txt\") as f: \n    context = f.read()\n\n\nSetup RLM\nsh = prep_shell(context, model=\"openai/openai/gpt-oss-120b\", base_url=\"https://your-litellm-gateway.com\")\nrun_repl = make_run_repl(sh)\n\n\nAsk a question\nquery = “What are the main themes discussed in this document?”\n\n\nRun RLM with verbose output\nresponses = advanced_toolloop( query, sp=REPL_SYSTEM_PROMPT, tools=[run_repl], sh=sh, model=\"openai/openai/gpt-oss-120b\", base_url=\"https://your-litellm-gateway.com\", max_steps=50, verbose=True)\n\n\nGet the answer\nfor item in responses:\n    if isinstance(item, dict) and item.get(\"type\") == \"final\":\n        print(f\"Answer: {item['answer']}\")\n\n\nVanialla approach ( may fail with very long context )\ntry:\n    vanilla_result = benchmark_vanilla(context, query, model=\"gpt-4\", base_url=\"...\")\n    print(f\"Vanilla: {vanilla_result['time']:.2f}s, {vanilla_result['tokens']} tokens\")\nexcept Exception as e:\n    print(f\"Vanilla failed: {e}\")\n\n\nRLM approach\nrlm_result = benchmark_rlm(context, query, model=\"gpt-4\", base_url=\"...\", verbose=True)\nprint(f\"RLM: {rlm_result['time']:.2f}s, {rlm_result['tokens']} tokens\")\nprint(f\"Answer: {rlm_result['answer']}\")",
    "crumbs": [
      "RLM - Recursive Language Models"
    ]
  },
  {
    "objectID": "index.html#faq",
    "href": "index.html#faq",
    "title": "RLM - Recursive Language Models",
    "section": "FAQ",
    "text": "FAQ\n\nWhen should I use RLM vs vanilla LLM?\nUse RLM when: - Context exceeds model’s window (100k+ tokens) - You experience “context rot” (model gets worse with long conversations) - Task requires reasoning across many documents - You want adaptive, test-time chunking strategies\nUse vanilla when: - Context is short (&lt; 10k tokens) - Simple fact retrieval - Speed is critical and context fits easily\n\n\nWhat’s the difference between max_steps and recursion depth?\n\nmax_steps: How many REPL iterations the root LM gets (horizontal - loop count)\nRecursion depth: How deep calls can nest (vertical - call stack depth)\n\nRLM enforces depth=1 by design: root LM can call llm_query(), but those calls can’t spawn further recursion.\n\n\nWhy doesn’t the model always use FINAL()?\nSome models don’t consistently follow the FINAL() instruction. RLM includes a fallback that captures the last assistant message if FINAL() isn’t detected.\n\n\nHow does RLM compare to RAG?\nRLM advantages: - No pre-indexing needed - Adaptive search strategies (model decides how to explore) - Better for complex multi-step reasoning\nRAG advantages: - Faster for simple lookups - Works well with persistent knowledge bases - Lower cost per query for repeated queries\n\n\nCan I customize the system prompt?\nYes! Import and modify REPL_SYSTEM_PROMPT or create your own:\nfrom rlm.prompts import REPL_SYSTEM_PROMPT\n\ncustom_prompt = REPL_SYSTEM_PROMPT + \"\\nAdditional instructions here...\"",
    "crumbs": [
      "RLM - Recursive Language Models"
    ]
  },
  {
    "objectID": "index.html#future-directions",
    "href": "index.html#future-directions",
    "title": "RLM - Recursive Language Models",
    "section": "Future Directions",
    "text": "Future Directions\nBased on the Recursive Language Models paper, here are planned enhancements:\n\nShort-term\n\nToken/cost tracking: Detailed metrics for each step\nMultiple benchmark tasks: Expand beyond document Q&A\nError recovery improvements: Better handling of API failures and malformed tool calls\nConfigurable FINAL detection: Custom patterns beyond FINAL() and FINAL_VAR()\n\n\n\nMedium-term\n\nTraining for recursion: Fine-tune models explicitly for RLM patterns (like o1 for reasoning)\nDeeper recursion: Support depth &gt; 1 for more complex tasks\nMulti-modal context: Support for images, tables, structured data\nStreaming responses: Real-time answer updates as RLM progresses\n\n\n\nLong-term\n\nRL-based optimization: Learn optimal chunking and recursion strategies\nHybrid RAG+RLM: Combine pre-indexed retrieval with adaptive exploration\nBenchmark suite: Comprehensive evaluation across domains",
    "crumbs": [
      "RLM - Recursive Language Models"
    ]
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "RLM - Recursive Language Models",
    "section": "Contributing",
    "text": "Contributing\nWe welcome contributions! Areas where help is needed: - Additional benchmark tasks - Prompt engineering for better FINAL() compliance - Performance optimizations - Documentation improvements",
    "crumbs": [
      "RLM - Recursive Language Models"
    ]
  }
]